{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the relevant packages\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import graphviz \n",
    "import pickle \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get the data! \n",
    "\n",
    "# The dataset we want to use originates from Kaggle. \n",
    "# \"The consumer credit department of a bank wants to automate the decisionmaking process for approval of \n",
    "#  home equity lines of credit.\"\n",
    "# \"The TARGET variable is a flag. \n",
    "#  If the value is a \"1\" then the person defaulted on the loan. \n",
    "#  If the value is a \"0\" then the person paid back the loan.\"\n",
    "# AIM: Develop a model to predict whether a customer will default on a loan.\n",
    "# The dataset can be downloaded at:\n",
    "# https://www.kaggle.com/ajay1735/hmeq-data/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Explore - Read the data into python dataframe and compute basic stats\n",
    "#         a. How many observations and how many features?\n",
    "#         b. Which variables are numerical and which are categorical?\n",
    "#         c. Min, max of numerical variables, levels of categorical variables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Rename the response variable, call it RiskPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Plot the distribution of features\n",
    "#         a. Plot all features in same plot\n",
    "#         b. Plot a selected feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Explore the distribution of missing data (both numbers and plots, missingno)\n",
    "#         a. Select a variable and see how it relates to the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Impute missing values of the numerical features using univariate imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Plot the distribution of the categorical features (boxplot with frequencies on y and levels on x)\n",
    "#         Make a function that you use for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Impute missing values using the most frequent term for each of the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Convert categorical variables to onehot representation\n",
    "#          (Unfortunately SKlearn do not handle categorical data directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Plot the correlation of all features:\n",
    "#          a. Pairs coloured by correlation value, see help-function below \n",
    "#          b. Scatter plot for all and for a selected pair\n",
    "# The following correlation plot is based on:\n",
    "# https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n",
    "def my_corr_plot( xdata, filename=None ):\n",
    "    fig, ax = plt.subplots(figsize=(10,10))  \n",
    "\n",
    "    # Using Pearson Correlation on numerical features\n",
    "    corr = xdata.corr()\n",
    "    ax = sns.heatmap(\n",
    "        corr, \n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        cmap=sns.diverging_palette(20, 220, n=200),\n",
    "        square=True\n",
    "    )\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=45,\n",
    "        horizontalalignment='right'\n",
    "    );\n",
    "    fig = ax.get_figure()\n",
    "    if( filename!=None ):\n",
    "        fig.savefig(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Split the data into y and X, check the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Use SKlearns feature_selction to explore variable importance, you can use \n",
    "#          the below plotting function for illustrating the results\n",
    "#          Based on: https://stackoverflow.com/questions/44101458/random-forest-feature-importance-chart-using-python\n",
    "def variable_importance_plot(scores,X):\n",
    "    # Get the indices sorted by most important to least important\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    N = X.shape[1]\n",
    "    features = []\n",
    "    for i in range(N):\n",
    "        features.append( X.columns[indices[i]])\n",
    "\n",
    "    # Now plot\n",
    "    plt.figure()\n",
    "    plt.barh(features, scores[indices[range(N)]], color='r', align='center')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Split into train (75%) and test sets (25%), check dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Fit a decision tree and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16: Evaluate the prediction accuracy on both train and test\n",
    "#          Compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17: Use joblib or pickle to save the model\n",
    "#          Test to load the model in a new notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18: Fit a random forest classifier \n",
    "#          Explore accuracy, confusion tables and variable importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 19: Sometimes it can be good to look at the predictions to see what is output\n",
    "#          Export both observations (y_test) and the correpsonding predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 20: Hyperparameter tuning of randomforest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 21: Rescale features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 22: Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 23: Logistic regression hyper-parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 24: Pipelines - finding best combination of preprocess and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 25: Pipeline - Fit several models with the same preprocessing step "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
